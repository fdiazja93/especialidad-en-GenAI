{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004f97b7",
   "metadata": {},
   "source": [
    "### Clase 4: Laboratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20055c4a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1987b189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Structured + Natural Text Dataset ===\n",
      "Loaded 339 characters.\n",
      "\n",
      "=== Training Models ===\n",
      "Epoch 0, RNN Loss: 184.8847, LSTM Loss: 216.9732\n",
      "Epoch 1, RNN Loss: 52.8883, LSTM Loss: 40.7044\n",
      "Epoch 2, RNN Loss: 46.0544, LSTM Loss: 35.0998\n",
      "Epoch 3, RNN Loss: 45.0642, LSTM Loss: 32.4935\n",
      "Epoch 4, RNN Loss: 43.8809, LSTM Loss: 32.1529\n",
      "\n",
      "=== Text Generation ===\n",
      "\n",
      "Generated text using RNN:\n",
      "trke he shin() se se cad_if } y\n",
      "whead() }\n",
      "while (): } { }\n",
      "whi }\n",
      "whi+ m }\n",
      "whe }\n",
      "whe } } sed(); }\n",
      "whe = }\n",
      "whind_in.\n",
      "whise) x aturke) sh in(); red_i+) }\n",
      "whe aturke); caturi+= }\n",
      "while m turere += re) } }\n",
      "whe }\n",
      "whe); } turkeathe) }\n",
      "whe } atururke m } aturke () () }\n",
      "whe == m = () she are saturke }\n",
      "tuse } }\n",
      "\n",
      "Generated text using LSTM:\n",
      "tum rke +); i; }\n",
      "whe rked(ushishif ad(e rdle == brd 0; { y_ogggggggggggggggggggggggggggggggggggg_i; }\n",
      "trn.\n",
      "f am isad aturke 0) { pe ske ; y\n",
      "trke she ndled(t rke { == > { }\n",
      "if boggggggggggggggggggggggggggggggggggggggedle and }\n",
      "whi; ath }\n",
      "whe_d() } x { ++ } ry care (x brn(edled r.\n",
      "de (x--; }\n",
      "whbond(t (\n",
      "\n",
      "=== Evaluation ===\n",
      "RNN Character Accuracy: 95.68%\n",
      "LSTM Character Accuracy: 96.96%\n",
      "RNN Perplexity: 1.18\n",
      "LSTM Perplexity: 1.09\n"
     ]
    }
   ],
   "source": [
    "# RNNs and LSTMs for NLP with BPTT\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Section 1: Load Structured Context + Hugging Face WikiText2\n",
    "print(\"\\n=== Loading Structured + Natural Text Dataset ===\")\n",
    "\n",
    "structured_lines = [\n",
    "    \"if (a == b) { return a; }\",\n",
    "    \"while (true) { break; }\",\n",
    "    \"for (i = 0; i < n; i++) { sum += i; }\",\n",
    "    \"def add(x, y): return x + y\",\n",
    "    \"while (x > 0) { x--; }\",\n",
    "    \"if (user.is_logged_in()) { show_dashboard(); }\",\n",
    "    \"try { risky_operation(); } catch (e) { handle_error(); }\",\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog barked at the moon.\",\n",
    "    \"The rain in Spain falls mainly on the plain.\"\n",
    "]\n",
    "\n",
    "structured_text = \"\\n\".join(structured_lines).lower()\n",
    "\n",
    "# Load WikiText2 from Hugging Face\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "raw_text = \"\\n\".join(wikitext[\"train\"][\"text\"]).lower()\n",
    "combined_text = structured_text #+ \"\\n\" + raw_text[:10]\n",
    "\n",
    "# Section 2: Prepare Dataset\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, seq_len=40):\n",
    "        self.chars = sorted(set(text))\n",
    "        self.char2idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx2char = {i: ch for ch, i in self.char2idx.items()}\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.data = [text[i:i+seq_len+1] for i in range(len(text) - seq_len)]\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        chunk = self.data[i]\n",
    "        x_str, y_str = chunk[:-1], chunk[1:]\n",
    "        x = torch.tensor([self.char2idx[c] for c in x_str])\n",
    "        y = torch.tensor([self.char2idx[c] for c in y_str])\n",
    "        return x, y\n",
    "\n",
    "seq_len = 40\n",
    "text = combined_text\n",
    "print(f\"Loaded {len(text)} characters.\")\n",
    "dataset = CharDataset(text, seq_len=seq_len)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Section 3: Define Models\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc(out)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out)\n",
    "\n",
    "# Initialize models\n",
    "vocab_size = dataset.vocab_size\n",
    "rnn_model = SimpleRNN(vocab_size, embed_dim=64, hidden_dim=128)\n",
    "lstm_model = LSTMModel(vocab_size, embed_dim=64, hidden_dim=128)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=0.005)\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=0.005)\n",
    "\n",
    "# Section 4: Training\n",
    "print(\"\\n=== Training Models ===\")\n",
    "\n",
    "for epoch in range(5):\n",
    "    rnn_loss, lstm_loss = 0, 0\n",
    "    for x, y in loader:\n",
    "        # RNN\n",
    "        rnn_out = rnn_model(x)\n",
    "        loss_rnn = criterion(rnn_out.view(-1, vocab_size), y.view(-1))\n",
    "        rnn_optimizer.zero_grad()\n",
    "        loss_rnn.backward()\n",
    "        rnn_optimizer.step()\n",
    "        rnn_loss += loss_rnn.item()\n",
    "\n",
    "        # LSTM\n",
    "        lstm_out = lstm_model(x)\n",
    "        loss_lstm = criterion(lstm_out.view(-1, vocab_size), y.view(-1))\n",
    "        lstm_optimizer.zero_grad()\n",
    "        loss_lstm.backward()\n",
    "        lstm_optimizer.step()\n",
    "        lstm_loss += loss_lstm.item()\n",
    "\n",
    "    print(f\"Epoch {epoch}, RNN Loss: {rnn_loss:.4f}, LSTM Loss: {lstm_loss:.4f}\")\n",
    "\n",
    "# Section 5: Text Generation with Sampling\n",
    "print(\"\\n=== Text Generation ===\")\n",
    "\n",
    "def sample_next_token(logits, temperature=1.0):\n",
    "    logits = logits / temperature\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "def generate_text(model, start_char, char2idx, idx2char, vocab_size, max_len=300, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_idx = torch.tensor([[char2idx[start_char]]])\n",
    "    result = [start_char]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(input_idx)\n",
    "            pred = output[:, -1, :]\n",
    "            next_idx = sample_next_token(pred.squeeze(), temperature)\n",
    "            next_char = idx2char[next_idx]\n",
    "            result.append(next_char)\n",
    "            input_idx = torch.tensor([[next_idx]])\n",
    "    return ''.join(result)\n",
    "\n",
    "print(\"\\nGenerated text using RNN:\")\n",
    "print(generate_text(rnn_model, start_char='t', char2idx=dataset.char2idx, idx2char=dataset.idx2char, vocab_size=vocab_size, temperature=0.8))\n",
    "\n",
    "print(\"\\nGenerated text using LSTM:\")\n",
    "print(generate_text(lstm_model, start_char='t', char2idx=dataset.char2idx, idx2char=dataset.idx2char, vocab_size=vocab_size, temperature=0.8))\n",
    "\n",
    "# Section 6: Evaluation\n",
    "print(\"\\n=== Evaluation ===\")\n",
    "\n",
    "def accuracy(model, data_loader, vocab_size):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            outputs = model(x)\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            correct += (predictions == y).sum().item()\n",
    "            total += y.numel()\n",
    "    return correct / total\n",
    "\n",
    "def perplexity(model, data_loader, vocab_size):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_words = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), y.view(-1))\n",
    "            total_loss += loss.item() * y.numel()\n",
    "            total_words += y.numel()\n",
    "    return math.exp(total_loss / total_words)\n",
    "\n",
    "print(f\"RNN Character Accuracy: {accuracy(rnn_model, loader, vocab_size):.2%}\")\n",
    "print(f\"LSTM Character Accuracy: {accuracy(lstm_model, loader, vocab_size):.2%}\")\n",
    "print(f\"RNN Perplexity: {perplexity(rnn_model, loader, vocab_size):.2f}\")\n",
    "print(f\"LSTM Perplexity: {perplexity(lstm_model, loader, vocab_size):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57fa38f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: if (a == b) {\n",
      "RNN completion:  if (a == b) { return a; }\n",
      "while (true) { handle_error(); }\n",
      "the cat sat on the mat.\n",
      "the dog barked at the moon.\n",
      "th\n",
      "LSTM completion: if (a == b) { return a; }\n",
      "while (x > 0) { x--; }\n",
      "if (user.is_logged_in())) { show_dashboard(); }\n",
      "try { risky_oper\n",
      "\n",
      "Prompt: while (x < 10) {\n",
      "RNN completion:  while (x < 10) { show_dashboard(); }\n",
      "try { risky_operation(); }\n",
      "the cat sat on the mat.\n",
      "the dog barked at the moon.\n",
      "\n",
      "LSTM completion: while (x < 10) { x--; }\n",
      "if (user.is_logged_in()) { show_dashboard(); }\n",
      "try { risky_operation(); } catch (e) { handle\n",
      "\n",
      "Prompt: for (i = 0; i < n; i++) {\n",
      "RNN completion:  for (i = 0; i < n; i++) { sum += i; }\n",
      "def add(x, y): return a; }\n",
      "while (true) { handle_error(); }\n",
      "the cat sat on the mat.\n",
      "the\n",
      "LSTM completion: for (i = 0; i < n; i++) { sum += i; }\n",
      "def add(x, y): return x + y\n",
      "while (x > 0) { x--; }\n",
      "if (user.is_logged_in()) { show_dash\n",
      "\n",
      "Prompt: try {\n",
      "RNN completion:  try { risky_operation(); }\n",
      "the cat sat on the mat.\n",
      "the dog barked at the moon.\n",
      "the rain in spain falls ma\n",
      "LSTM completion: try { risky_operation(); } catch (e) { handle_error(); }\n",
      "the cat sat on the mat.\n",
      "the dog barkedd at the m\n",
      "\n",
      "Prompt: The cat\n",
      "RNN completion:  the cat sat on(); }\n",
      "the cat sat on the mat.\n",
      "the dog barked at the moon.\n",
      "the rain in spain falls mainly on t\n",
      "LSTM completion: the cat sat on the mat.\n",
      "the dog barked at the moon.\n",
      "the rain in spain falls mainly on the plain.\n",
      "the rain i\n"
     ]
    }
   ],
   "source": [
    "def complete_prompt(model, prompt, char2idx, idx2char, vocab_size, max_len=100, temperature=0.8):\n",
    "    model.eval()\n",
    "    result = list(prompt)\n",
    "    input_idx = torch.tensor([[char2idx[c] for c in prompt if c in char2idx]])\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(input_idx)\n",
    "            pred = output[:, -1, :]\n",
    "            next_idx = sample_next_token(pred.squeeze(), temperature)\n",
    "            next_char = idx2char[next_idx]\n",
    "            result.append(next_char)\n",
    "            input_idx = torch.tensor([[char2idx[c] for c in result[-seq_len:] if c in char2idx]])\n",
    "    return ''.join(result)\n",
    "example_prompts = [\n",
    "    \"if (a == b) {\",\n",
    "    \"while (x < 10) {\",\n",
    "    \"for (i = 0; i < n; i++) {\",\n",
    "    \"try {\",\n",
    "    \"The cat\"\n",
    "]\n",
    "\n",
    "for prompt in example_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    rnn_completion = complete_prompt(rnn_model, prompt.lower(), dataset.char2idx, dataset.idx2char, vocab_size)\n",
    "    lstm_completion = complete_prompt(lstm_model, prompt.lower(), dataset.char2idx, dataset.idx2char, vocab_size)\n",
    "    print(f\"RNN completion:  {rnn_completion}\")\n",
    "    print(f\"LSTM completion: {lstm_completion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1d8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
